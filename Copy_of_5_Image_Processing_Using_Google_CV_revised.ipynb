{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 5. Image Processing Using Google CV_revised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Starrysmh/MARK5828-AdvertisingAnalytics-Project/blob/main/Copy_of_5_Image_Processing_Using_Google_CV_revised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feJUEpyYD05Q"
      },
      "source": [
        "# What you need for running this notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E859P_hIzaoL"
      },
      "source": [
        "1.   Google Cloud Platform (GCP) Account & the json_file with keys/credentials \n",
        "2.   Images downloaded from Instagram (if you ran the notebook 3. Downloading Images from Instagram, these would be saved in your g-drive/Advertising Analytics/Instagram_Images)\n",
        "3.   Active Google Drive Account\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhpaBgjEXze"
      },
      "source": [
        "# The Vision API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LR-QjtAPC9D"
      },
      "source": [
        "In this colab notebook, We will see how to use call the following Google Vision APIs and extract the essential data and store it in a csv format:\n",
        "\n",
        "\n",
        "1.   Optical Character Recognition\n",
        "1.   Face Detection\n",
        "1.   Landmark Detection\n",
        "1.   Safe Search Detection\n",
        "2.   Multiple Object Detection\n",
        "2.   Image Properties Detection\n",
        "2.   Web Entities Detection\n",
        "2.   Logo Detection\n",
        "\n",
        "\n",
        "For more Information please see the website below to understand these apis :\n",
        "https://cloud.google.com/vision/docs/features-list#image-properties-5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X096BVX1QJSb"
      },
      "source": [
        "Please mount your google drive when prompted later so that the data can be stored safely and conveniently there . The program expects access to google drive to store the extracted data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xT0nYOtEhg6"
      },
      "source": [
        "#Installing/Importing the Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ju2M6oaIl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139be4c7-212f-4e25-d621-361b7d707f57"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import cv2\n",
        "import io\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "try:\n",
        "  import colour\n",
        "except ModuleNotFoundError:\n",
        "  !pip install colour\n",
        "  import colour"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colour\n",
            "  Downloading https://files.pythonhosted.org/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl\n",
            "Installing collected packages: colour\n",
            "Successfully installed colour-0.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrHJI7mYnQVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fea4f02-3767-4b64-da1c-36fc3fcc0b4b"
      },
      "source": [
        "!pip install --upgrade google-cloud-vision\n",
        "!pip install --upgrade google-api-python-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/84/963344216d11e095995b20b55b5af3739db03439be7dfdf265de5271c714/google_cloud_vision-2.3.2-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 4.2MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from proto-plus>=1.15.0->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.34.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.2 proto-plus-1.18.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting google-api-python-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/8b/98c96583e0c1fcbe0aba60cd37e50768893df59d18249ab12d6195605423/google_api_python_client-2.10.0-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.26.3)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.17.4)\n",
            "Collecting google-auth-httplib2>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/db/721e2f3f32339080153995d16e46edc3a7657251f167ddcb9327e632783b/google_auth_httplib2-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
            "\u001b[31mERROR: earthengine-api 0.1.269 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 2.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth-httplib2, google-api-python-client\n",
            "  Found existing installation: google-auth-httplib2 0.0.4\n",
            "    Uninstalling google-auth-httplib2-0.0.4:\n",
            "      Successfully uninstalled google-auth-httplib2-0.0.4\n",
            "  Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "Successfully installed google-api-python-client-2.10.0 google-auth-httplib2-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9irrxwIMOKX9"
      },
      "source": [
        "from google.cloud import vision\n",
        "from google.cloud import vision_v1\n",
        "#from google.cloud.vision_v1 import enums\n",
        "#from google.cloud.vision import types\n",
        "from google.cloud.vision_v1 import types\n",
        "from google.protobuf.json_format import MessageToJson\n",
        "import json\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXcTWlWEpdR"
      },
      "source": [
        "# Connecting to Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umMGEacY44z6"
      },
      "source": [
        "To access the Google Cloud APIs, we need to upload our unique credentials and establish the link to our accounts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xWMotHH3lIJ",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "8d32bf87-594d-4592-c0e9-7d3662d1274d"
      },
      "source": [
        "print('Please upload the Json file with keys/credentials for google cloud apis:')\n",
        "key_json=files.upload()\n",
        "client = vision.ImageAnnotatorClient.from_service_account_json(list(key_json.keys())[0])\n",
        "os.remove(list(key_json.keys())[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please upload the Json file with keys/credentials for google cloud apis:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-20c84c5e-aaa0-4262-aadc-9d6ea4c24ade\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-20c84c5e-aaa0-4262-aadc-9d6ea4c24ade\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving key_json.json to key_json.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl1rselsE3H7"
      },
      "source": [
        "# Mount the Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67WYQrLQQglz"
      },
      "source": [
        "**Please Mount the google drive now!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW492wykp91K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7fd8f0-77e6-43ea-b89b-782a732efd13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZN9Hfb6P7Im"
      },
      "source": [
        "# Setting up paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beFzKXjsP6Nl"
      },
      "source": [
        "json_output_path='./drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/json_output/'\n",
        "formatted_json_path='./drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/Formatted_Json/'\n",
        "extracted_vision_data_path='./drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/extracted_vision_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4JYXyy1LD7d"
      },
      "source": [
        "# Clearing up the Folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIA7i6beLBsp"
      },
      "source": [
        "This section aims to delete all the files in the following g-drive folders and clear them up\n",
        "1. Advertising Analytics/json_output\n",
        "2. Advertising Analytics/Formatted_Json\n",
        "3. Advertising Analytics/extracted_vision_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZH_3ucOLBJc"
      },
      "source": [
        "#json_output_path='./drive/My Drive/Advertising Analytics/json_output/'\n",
        "#formatted_json_path='./drive/My Drive/Advertising Analytics/Formatted_Json/'\n",
        "# extracted_vision_data_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'\n",
        "def delete_all_files():\n",
        "  try:\n",
        "    print(f'{len(os.listdir(json_output_path))} files are about to be deleted from the folder {json_output_path}')\n",
        "    for i in list(os.listdir(json_output_path)):\n",
        "      os.remove(json_output_path+i)\n",
        "  except FileNotFoundError:\n",
        "    print('Nothing to delete: The folder json_output does not exist!!')\n",
        "  try:\n",
        "    print(f'{len(os.listdir(formatted_json_path))} files are about to be deleted from the folder {formatted_json_path}')\n",
        "    for i in list(os.listdir(formatted_json_path)):\n",
        "      os.remove(formatted_json_path+i)\n",
        "  except FileNotFoundError:\n",
        "    print('Nothing to delete: The folder Formatted_Json does not exist!!')\n",
        "  try:\n",
        "    print(f'{len(os.listdir(extracted_vision_data_path))} files are about to be deleted from the folder {extracted_vision_data_path}')\n",
        "    for i in list(os.listdir(extracted_vision_data_path)):\n",
        "      os.remove(extracted_vision_data_path+i)\n",
        "  except FileNotFoundError:\n",
        "    print('Nothing to delete: The folder extracted_vision_data does not exist!!')\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADt_CFNqPK9P"
      },
      "source": [
        "DON'T RUN THE BELOW LINE UNLESS YOU WANT TO CLEAR THE FOLDERS !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHKfTd-qNLIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777f990f-5761-4c15-95a8-134791e7a0f2"
      },
      "source": [
        "delete_all_files()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nothing to delete: The folder json_output does not exist!!\n",
            "Nothing to delete: The folder Formatted_Json does not exist!!\n",
            "Nothing to delete: The folder extracted_vision_data does not exist!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdUqin7N7MpH"
      },
      "source": [
        "# Resizing Larger Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiL-XhbO7XeD"
      },
      "source": [
        "We have added a function that checks for the size of the images.As long as there is an image of more than 900 KB, the function will keep on down-sizing that image. \n",
        "\n",
        "This should help avoid the exceptions from the Google API that some of you were facing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "-7iTMPWCY_qC",
        "outputId": "a32881db-8537-4a4a-dabc-717983141d05"
      },
      "source": [
        "list(os.listdir(img_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2ea07b066571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './content/drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/Instagram_Images/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4ZnwrzA7TPl"
      },
      "source": [
        "img_path='./drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/Instagram_Images/'\n",
        "def resize_images(img_path=img_path):\n",
        "  size_dict={i:os.stat(img_path+i).st_size/1024_000 for i in list(os.listdir(img_path))}\n",
        "  oversized_images={i for i in size_dict.keys() if size_dict[i]>=0.9}\n",
        "  if len(oversized_images)>0:\n",
        "    for i in oversized_images:\n",
        "      img = cv2.imread(img_path+i, cv2.IMREAD_UNCHANGED)\n",
        "      scale_percent = 80 # percent of original size\n",
        "      width = int(img.shape[1] * scale_percent / 100)\n",
        "      height = int(img.shape[0] * scale_percent / 100)\n",
        "      dim=(width,height)\n",
        "      resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "      cv2.imwrite(img_path+i,resized)\n",
        "    time.sleep(5)\n",
        "    resize_images()\n",
        "  else:\n",
        "    return 'Done, huge images have been resized!'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip_K1pVf8hmz"
      },
      "source": [
        "resize_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmRSaTTEFNKI"
      },
      "source": [
        "# Making Call(s) to the Vision API(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvbdZfqGRJAu"
      },
      "source": [
        "\n",
        "\n",
        "*  We have defined a function below to make calls to the vision apis and store the outputs. The function takes in 16 images at a time (one batch) and calls the all the apis of interest.\n",
        "*  The outputs are stored in Json Format. As such in their raw formats, the json files can be difficult to read.\n",
        "*  We convert these files to a more readable format and save them for future use\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52xXnv940rj"
      },
      "source": [
        "Restrictions of Google API Trial Account:\n",
        "\n",
        "\n",
        "1. Input Restrictions --> Not More than 20 MB (If our batch of 16 images exceeds 20MB, API would throw an error).\n",
        "2. Output Restrictions --> If the output json_files for a batch is more than 10 MB, API would throw an error).\n",
        "3. Limited Calls to the API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-519a9vBXVlq"
      },
      "source": [
        "  def get_results_from_vision_api(batch_size=16):\n",
        "    \n",
        "    features = [\n",
        "        {\"type_\": vision_v1.Feature.Type.TEXT_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.FACE_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.LABEL_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.IMAGE_PROPERTIES},\n",
        "        {\"type_\": vision_v1.Feature.Type.LOGO_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.OBJECT_LOCALIZATION},\n",
        "        {\"type_\": vision_v1.Feature.Type.LANDMARK_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.SAFE_SEARCH_DETECTION},\n",
        "        {\"type_\": vision_v1.Feature.Type.WEB_DETECTION}\n",
        "    ]\n",
        "    jpg_list=[i for i in os.listdir(img_path) if i.endswith('jpg')]\n",
        "    n_batches=[]\n",
        "    image_batch=[]\n",
        "    batch_response=[]\n",
        "    l=list(range(0,len(jpg_list),batch_size))\n",
        "\n",
        "    #Forming Batches for api calls\n",
        "    for i in range(len(l)):\n",
        "      if i!=len(l)-1:\n",
        "        n_batch=list(range(l[i],l[i+1]))\n",
        "        #n_batches.append(list(range(l[i],l[i+1])))\n",
        "        image_batch.append([jpg_list[k] for k in n_batch])\n",
        "      else:\n",
        "        #n_batches.append(list(range(l[-1],len(jpg_list))))\n",
        "        n_batch=list(range(l[-1],len(jpg_list)))\n",
        "        image_batch.append([jpg_list[k] for k in n_batch])\n",
        "    \n",
        "    for j in range(len(image_batch)):\n",
        "      img_list=image_batch[j]\n",
        "      requests=[]\n",
        "      for filename in img_list:\n",
        "        try:\n",
        "          with open(img_path+filename,'rb') as image_file:\n",
        "            image=types.Image(content=image_file.read())\n",
        "          request=types.AnnotateImageRequest(image=image,features=features)\n",
        "          requests.append(request)\n",
        "        except FileNotFoundError:\n",
        "          continue\n",
        "          print('Some Images seem to be corrupt! Please check!!')\n",
        "#        except :\n",
        "#          continue\n",
        "      try:\n",
        "        batch_response=client.batch_annotate_images(requests=requests)\n",
        "      except:#GCP restricts the output json file size to be not more than 10 MB per batch (16 images) This exception is to ignore such error and continue \n",
        "          print('A batch had exceeded Payload output for the trial account! It has been skipped for the time being')\n",
        "          continue\n",
        "          \n",
        "      for r in range(len(batch_response.responses)):\n",
        "        if batch_response.responses[r].error.message:\n",
        "          # raise Exception(\n",
        "          #       '{}\\nFor more info on error messages, check: '\n",
        "          #       'https://cloud.google.com/apis/design/errors'.format(\n",
        "          #           batch_response.responses[r].error.message))\n",
        "          continue\n",
        "        else:\n",
        "          try:\n",
        "             with open(json_output_path+img_list[r].split('.jpg')[0]+'.json','w') as fp:\n",
        "               json.dump(type(batch_response.responses[r]).to_json(batch_response.responses[r]),fp,indent=4) \n",
        "          except FileNotFoundError:\n",
        "            os.mkdir(json_output_path)\n",
        "            with open(json_output_path+img_list[r].split('.jpg')[0]+'.json','w') as fp:\n",
        "               json.dump(type(batch_response.responses[r]).to_json(batch_response.responses[r]),fp,indent=4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHOz7lnuad-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78405c5-93d0-4bbd-8d1f-fa34379e26ea"
      },
      "source": [
        "get_results_from_vision_api()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A batch had exceeded Payload output for the trial account! It has been skipped for the time being\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGkte3RJ5pNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca9fa02-d2c7-4f6c-e147-2f6d2536021a"
      },
      "source": [
        "json_list=[i for i in os.listdir(json_output_path)]\n",
        "print(f'We have {len(json_list)} json files generated from the vision API')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 1394 json files generated from the vision API\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ueHNgBw5g8n"
      },
      "source": [
        "Formatting our json outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfbTfaw-kJ-5"
      },
      "source": [
        "def beautify_json(input_json,src_path=json_output_path,dest_path=formatted_json_path):\n",
        "\n",
        "    json_dict={}\n",
        "    with open(src_path+'/'+input_json) as json_inp:\n",
        "        json_data = json.load(json_inp)\n",
        "    json_file_name=input_json.split('.json')[0]\n",
        "    json_dict[json_file_name]=json.loads(json_data)\n",
        "\n",
        "    try:\n",
        "      with open(dest_path+'/'+json_file_name+str('_pretty.json'),'w') as json_inp:\n",
        "        json.dump(json_dict[json_file_name],json_inp,indent=4)\n",
        "    except FileNotFoundError:\n",
        "      os.mkdir(dest_path)\n",
        "      with open(dest_path+'/'+json_file_name+str('_pretty.json'),'w') as json_inp:\n",
        "        json.dump(json_dict[json_file_name],json_inp,indent=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOMqer7g59tK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "c4b7cfa2-78cf-4896-dbb9-33aa7f017d4d"
      },
      "source": [
        "#Iterating over the list of json files and formatting them\n",
        "for i in json_list:\n",
        "  beautify_json(input_json=i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e0215d94e530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Iterating over the list of json files and formatting them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mbeautify_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-dde51876545c>\u001b[0m in \u001b[0;36mbeautify_json\u001b[0;34m(input_json, src_path, dest_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_inp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mjson_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './drive/MyDrive/Advertising_Analytics_T2_2021/B2C_B2B/gsus4/json_output//.ipynb_checkpoints'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDSz4NbSTi7"
      },
      "source": [
        "# Extracting the Data from the Json Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pLYkFiQSPqa"
      },
      "source": [
        "\n",
        "*   We have defined various functions below (hidden intentionally) to read the json files and extract the relevant features\n",
        "*   We store these features in a data frame and then write it to a csv file which would be saved to g-drive/Advertising Analytics/extracted_vison_data/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjOE4oEdJkmE"
      },
      "source": [
        "src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/'\n",
        "dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj61ovhF80AM"
      },
      "source": [
        "**The following block of code is hidden intentionally! BUT Don't forget to run it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CHXRDFJzKW0"
      },
      "source": [
        "#@title\n",
        "def read_json(json_file):\n",
        "    with open(json_file,'r') as json_inp:\n",
        "        json_data=json.load(json_inp)\n",
        "    return json_data\n",
        "\n",
        "def OCR_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        text_features=json_data['textAnnotations']\n",
        "        desc=[]\n",
        "        locale=[]\n",
        "        for t in range(len(text_features)):\n",
        "            if t==0:\n",
        "                locale.append(text_features[t]['locale'])\n",
        "            desc.append(text_features[t]['description'])\n",
        "        #locale=locale*len(desc)\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(locale)\n",
        "        text_features_dict={'file':file_name,'locale':locale,'ocr_text_description':' '.join(desc)}\n",
        "        return text_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any texts!\")\n",
        "        return []\n",
        "    \n",
        "def Logo_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        logo_features=json_data['logoAnnotations']\n",
        "        desc=[]\n",
        "        score=[]\n",
        "        for t in range(len(logo_features)):\n",
        "            desc.append(logo_features[t]['description'])\n",
        "            score.append(logo_features[t]['score'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(score)\n",
        "        logo_features_dict={'file':file_name,'logo_description':desc,'logo_score':score}\n",
        "        return logo_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any logos!\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def Label_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file=json_file)\n",
        "    try:\n",
        "        label_features=json_data['labelAnnotations']\n",
        "        desc=[]\n",
        "        score=[]\n",
        "        topicality=[]\n",
        "        for t in range(len(label_features)):\n",
        "            desc.append(label_features[t]['description'])\n",
        "            score.append(label_features[t]['score'])\n",
        "            topicality.append(label_features[t]['topicality'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(score)\n",
        "        label_features_dict={'file':file_name,'label_description':desc,'label_score':score,'topicality':topicality}\n",
        "        return label_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any labels!\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def Face_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        face_features=json_data['faceAnnotations']\n",
        "        anger=[]\n",
        "        sorrow=[]\n",
        "        joy=[]\n",
        "        surprise=[]\n",
        "\n",
        "        blurred=[]\n",
        "        underexposed=[]\n",
        "        headwear=[]\n",
        "\n",
        "        detection_confidence=[]\n",
        "\n",
        "        for t in range(len(face_features)):\n",
        "            anger.append(face_features[t]['angerLikelihood'])\n",
        "            sorrow.append(face_features[t]['sorrowLikelihood'])\n",
        "            joy.append(face_features[t]['joyLikelihood'])\n",
        "            surprise.append(face_features[t]['surpriseLikelihood'])\n",
        "            blurred.append(face_features[t]['blurredLikelihood'])\n",
        "            underexposed.append(face_features[t]['underExposedLikelihood'])\n",
        "            headwear.append(face_features[t]['headwearLikelihood'])\n",
        "            detection_confidence.append(face_features[t]['detectionConfidence'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(detection_confidence)\n",
        "        face_features_dict={'file':file_name,'anger':anger,'sorrow':sorrow,'joy':joy,'surprise':surprise,\n",
        "        'blurred':blurred,'underexposed':underexposed,'headwear':headwear,'detection_confidence':detection_confidence}\n",
        "        return face_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any faces!!\")\n",
        "        return []\n",
        "\n",
        "def Image_Properties_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        color_features=json_data['imagePropertiesAnnotation']['dominantColors']['colors']\n",
        "        color=[]\n",
        "        pixel_fraction=[]\n",
        "        score=[]\n",
        "        for t in range(len(color_features)):\n",
        "            color.append((color_features[t]['color']['red'],color_features[t]['color']['green'],color_features[t]['color']['blue']))\n",
        "            pixel_fraction.append(color_features[t]['pixelFraction'])\n",
        "            score.append(color_features[t]['score'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(score)\n",
        "        color_features_dict={'file':file_name,'color':color,'pixel_fraction':pixel_fraction,'color_score':score}\n",
        "        return color_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any colors!!\")\n",
        "        return []\n",
        "\n",
        "def Landmark_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        landmark_features=json_data['landmarkAnnotations']\n",
        "        desc=[]\n",
        "        score=[]\n",
        "        for t in range(len(landmark_features)):\n",
        "            desc.append(landmark_features[t]['description'])\n",
        "            score.append(landmark_features[t]['score'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(score)\n",
        "        landmark_features_dict={'file':file_name,'landmark_description':desc,'landmark_score':score}\n",
        "        return landmark_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have any landmarks!!\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def Safe_Search_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        safe_search_features=json_data['safeSearchAnnotation']\n",
        "\n",
        "        safe_search_features_dict={'file':json_file.split('.json')[0].split('/')[-1],'adult':[safe_search_features['adult']],\n",
        "                                   'medical':[safe_search_features['medical']],\n",
        "                                   'racy':[safe_search_features['racy']],\n",
        "                                   'spoof':[safe_search_features['spoof']],\n",
        "                                   'violence':[safe_search_features['violence']]\n",
        "                                   }\n",
        "        return safe_search_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the api failed at this classification of {json_file.split('.json')[0].split('/')[-1]}!!\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def Web_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        web_entities=json_data['webDetection']['webEntities']\n",
        "        desc=[]\n",
        "        score=[]\n",
        "        best_guess_labels=[]\n",
        "        for k in range(len(json_data['webDetection']['bestGuessLabels'])):\n",
        "            best_guess_labels.append(json_data['webDetection']['bestGuessLabels'][k]['label'])\n",
        "        for t in range(len(web_entities)):\n",
        "\n",
        "            try:\n",
        "              desc.append(web_entities[t]['description'])\n",
        "              score.append(web_entities[t]['score'])\n",
        "            except KeyError:\n",
        "              if len(desc)>len(score):\n",
        "                desc.pop(-1)\n",
        "              elif len(score)>len(desc):\n",
        "                score.pop(-1)\n",
        "              else:\n",
        "                continue\n",
        "\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(score)\n",
        "        best_guess_labels=best_guess_labels*len(score)\n",
        "        web_features_dict={'file':file_name,'best_guess_labels':best_guess_labels,'web_description':desc,'web_score':score}\n",
        "        return web_features_dict\n",
        "        \n",
        "    except KeyError:\n",
        "        print(f\"Seems like the api could not associate the {json_file.split('.json')[0].split('/')[-1]} with any web entities\")\n",
        "        return []\n",
        "\n",
        "def Multiple_Objects_Features_Extract(json_file):\n",
        "    json_data=read_json(json_file)\n",
        "    try:\n",
        "        multiple_objects_features=json_data['localizedObjectAnnotations']\n",
        "        name=[]\n",
        "        score=[]\n",
        "        for t in range(len(multiple_objects_features)):\n",
        "            name.append(multiple_objects_features[t]['name'])\n",
        "            score.append(multiple_objects_features[t]['score'])\n",
        "        file_name=[json_file.split('.json')[0].split('/')[-1]]*len(name)\n",
        "\n",
        "        multiple_objects_features_dict={'file':file_name,'object_name':name,'object_score':score}\n",
        "        return multiple_objects_features_dict\n",
        "    except KeyError:\n",
        "        print(f\"Seems like the {json_file.split('.json')[0].split('/')[-1]} didn't have multiple objects!\")\n",
        "        return []\n",
        "\n",
        "# def Crop_Hints_Features_Extract(json_file):\n",
        "#     json_data=read_json(json_file)\n",
        "#     try:\n",
        "#         crop_hints_features=json_data['cropHintsAnnotation']['cropHints']\n",
        "#         crop_confidence=[]\n",
        "#         crop_importance_fraction=[]\n",
        "#         for t in range(len(crop_hints_features)):\n",
        "#             crop_confidence.append(crop_hints_features[t]['confidence'])\n",
        "#             crop_importance_fraction.append(crop_hints_features[t]['importanceFraction'])\n",
        "#         file_name=[json_file.split('.json')[0].split('/')[-1]]*len(crop_confidence)\n",
        "#         crop_hints_features_dict={'file':file_name,'crop_confidence':crop_confidence,'crop_importance_fraction':crop_importance_fraction}\n",
        "#         return crop_hints_features_dict\n",
        "\n",
        "#     except KeyError:\n",
        "#         print(f\"Seems like the api could not find much for {json_file.split('.json')[0].split('/')[-1]}!\")\n",
        "#         return []\n",
        "##Downloading\n",
        "##Downloading\n",
        "def ocr_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  ocr_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=OCR_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      ocr_data=ocr_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    ocr_data.to_excel(dest_path+'/'+'ocr_data.xlsx',index=None)\n",
        "    files.download(dest_path+'ocr_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    ocr_data.to_excel(dest_path+\"/\"+'ocr_data.xlsx',index=None)\n",
        "    files.download(dest_path+'ocr_data.xlsx')\n",
        "\n",
        "def logo_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  logo_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Logo_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      logo_data=logo_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    logo_data.to_excel(dest_path+'/'+'logo_data.xlsx',index=None)\n",
        "    files.download(dest_path+'logo_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    logo_data.to_excel(dest_path+\"/\"+'logo_data.xlsx',index=None)\n",
        "    files.download(dest_path+'logo_data.xlsx')\n",
        "\n",
        "def face_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  face_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Face_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      face_data=face_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    face_data.to_excel(dest_path+'/'+'face_data.xlsx',index=None)\n",
        "    files.download(dest_path+'face_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    face_data.to_excel(dest_path+\"/\"+'face_data.xlsx',index=None)\n",
        "    files.download(dest_path+'face_data.xlsx')\n",
        "\n",
        "def landmark_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  landmark_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Landmark_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      landmark_data=landmark_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    landmark_data.to_excel(dest_path+'/'+'landmark_data.xlsx',index=None)\n",
        "    files.download(dest_path+'landmark_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    landmark_data.to_excel(dest_path+\"/\"+'landmark_data.xlsx',index=None)\n",
        "    files.download(dest_path+'landmark_data.xlsx')\n",
        "\n",
        "def image_properties_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  image_properties_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Image_Properties_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      image_properties_data=image_properties_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    image_properties_data.to_excel(dest_path+'/'+'image_properties_data.xlsx',index=None)\n",
        "    files.download(dest_path+'image_properties_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    image_properties_data.to_excel(dest_path+\"/\"+'image_properties_data.xlsx',index=None)\n",
        "    files.download(dest_path+'image_properties_data.xlsx')\n",
        "\n",
        "\n",
        "def safe_search_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  safe_search_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Safe_Search_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      safe_search_data=safe_search_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    safe_search_data.to_excel(dest_path+'/'+'safe_search_data.xlsx',index=None)\n",
        "    files.download(dest_path+'safe_search_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    safe_search_data.to_excel(dest_path+\"/\"+'safe_search_data.xlsx',index=None)\n",
        "    files.download(dest_path+'safe_search_data.xlsx')\n",
        "\n",
        "def web_search_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  web_search_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Web_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      web_search_data=web_search_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    web_search_data.to_excel(dest_path+'/'+'web_search_data.xlsx',index=None)\n",
        "    files.download(dest_path+'web_search_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    web_search_data.to_excel(dest_path+\"/\"+'web_search_data.xlsx',index=None)\n",
        "    files.download(dest_path+'web_search_data.xlsx') \n",
        "\n",
        "\n",
        "def multiple_objects_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  multiple_objects_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Multiple_Objects_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      multiple_objects_data=multiple_objects_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    multiple_objects_data.to_excel(dest_path+'/'+'multiple_objects_data.xlsx',index=None)\n",
        "    files.download(dest_path+'multiple_objects_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    multiple_objects_data.to_excel(dest_path+\"/\"+'multiple_objects_data.xlsx',index=None)\n",
        "    files.download(dest_path+'multiple_objects_data.xlsx')\n",
        "\n",
        "# def crop_hints_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "#   crop_hints_data=pd.DataFrame()\n",
        "#   for j in os.listdir(src_path):\n",
        "#     z=Crop_Hints_Features_Extract(json_file=src_path+j)\n",
        "#     if z!=[]:\n",
        "#       z=pd.DataFrame.from_dict(z)\n",
        "#       crop_hints_data=crop_hints_data.append(z,ignore_index=True)\n",
        "#   try:\n",
        "#     os.listdir(dest_path)\n",
        "#     crop_hints_data.to_excel(dest_path+'/'+'crop_hints_data.xlsx',index=None)\n",
        "#     files.download(dest_path+'crop_hints_data.xlsx')\n",
        "#   except FileNotFoundError:\n",
        "#     os.mkdir(dest_path)\n",
        "#     crop_hints_data.to_excel(dest_path+\"/\"+'crop_hints_data.xlsx',index=None)\n",
        "#     files.download(dest_path+'crop_hints_data.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def label_data_download(src_path='./drive/My Drive/Advertising Analytics/Formatted_Json/',dest_path='./drive/My Drive/Advertising Analytics/extracted_vision_data/'):\n",
        "  label_data=pd.DataFrame()\n",
        "  for j in os.listdir(src_path):\n",
        "    z=Label_Features_Extract(json_file=src_path+j)\n",
        "    if z!=[]:\n",
        "      z=pd.DataFrame.from_dict(z)\n",
        "      label_data=label_data.append(z,ignore_index=True)\n",
        "  try:\n",
        "    os.listdir(dest_path)\n",
        "    label_data.to_excel(dest_path+'/'+'label_data.xlsx',index=None)\n",
        "    files.download(dest_path+'label_data.xlsx')\n",
        "  except FileNotFoundError:\n",
        "    os.mkdir(dest_path)\n",
        "    label_data.to_excel(dest_path+\"/\"+'label_data.xlsx',index=None)\n",
        "    files.download(dest_path+'label_data.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDDX3-oGI32I"
      },
      "source": [
        "# Downloading the Extracted Data as CSVs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeOXSYCfOZWC"
      },
      "source": [
        "The following section aims to download the data extracted from the API as CSVs. It would automatically be saved in the g-drive as well in the folder g-drive/Advertising Analytics/extracted_vision_data/\n",
        "\n",
        "Please note that you may not need to run all the following commands. Based on your research question, you may choose to run only selective commands.\n",
        "\n",
        "Note: You will end up downloading an empty file in case the API didn't give any outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNwiA9t1Hn33"
      },
      "source": [
        "1. Optical Character Recognition (OCR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vu9esNuHtpJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9f84a768-96b3-4259-dd18-0723adfcb3e0"
      },
      "source": [
        "ocr_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_252962ad-ad4b-4daf-a6ee-36dafc6e4ee2\", \"ocr_data.xlsx\", 5734)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8NKtQeHukL"
      },
      "source": [
        "2. Logo Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q52SH9t_Hx_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d4c234e7-24b8-4bea-9798-6eec85c079ad"
      },
      "source": [
        "logo_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3f4bb007-931e-496b-b31f-b8faf437840a\", \"logo_data.xlsx\", 5399)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vq5F-S_Hzx7"
      },
      "source": [
        "3. Face Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUyMlez1H4mQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7f63f258-a16f-46c8-b9eb-bac8423d61b0"
      },
      "source": [
        "face_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0c2c82a0-9cfb-46a0-ae19-8aa2b521b845\", \"face_data.xlsx\", 5252)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx4SgOoEH7fF"
      },
      "source": [
        "4. Multiple Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN6BnmnOIJvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0438e275-0cc2-4a97-ce17-18e160288955"
      },
      "source": [
        "multiple_objects_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_54e385eb-c7d1-42a0-a5cc-86e40cf280b2\", \"multiple_objects_data.xlsx\", 5944)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F047vHJHIL-V"
      },
      "source": [
        " 5. Image Properties Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s3ony_DIScY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a80a16f7-89fe-4e88-9175-c3ec075b5df3"
      },
      "source": [
        "image_properties_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3d7a3009-ebf7-4728-b678-bda101da8aaf\", \"image_properties_data.xlsx\", 12382)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n04D_lNoIUrB"
      },
      "source": [
        "6. Web Entities Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WLBYZsGIbSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "06667d4e-42f5-42a0-fee1-6940cb656f5f"
      },
      "source": [
        "web_search_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f2c18073-c248-4ea1-9bbc-0ffee0b51e39\", \"web_search_data.xlsx\", 10224)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkDBB9NaIdhm"
      },
      "source": [
        "7. Safe Search Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78yldqYrIf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "61ee8af7-4ee7-43d9-e63e-5bdb8432645c"
      },
      "source": [
        "safe_search_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b88f28d5-27d9-4eba-9ce9-88ff5444c2c6\", \"safe_search_data.xlsx\", 5581)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-4AcvsbIhvq"
      },
      "source": [
        "8. Landmark Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI6w8IlrIkYo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ae53d1f9-340d-4772-a298-bebd65497d9d"
      },
      "source": [
        "landmark_data_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3cbf162e-7cce-495d-bf1f-345e32d32991\", \"landmark_data.xlsx\", 4955)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3k5i1kiAYuo"
      },
      "source": [
        "9. Label Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX4pznPMAYLG"
      },
      "source": [
        "label_data_download()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}